---
sidebar_position: 1
title: Lesson 3.1 - NVIDIA Isaac Sim and Synthetic Data
---

# Lesson 3.1: NVIDIA Isaac Sim and Synthetic Data

**Duration**: ~3 hours | **Pages**: 20 | **Difficulty**: Advanced

## Learning Objectives

By the end of this lesson, you will:
- âœ… Understand Isaac Sim architecture and GPU-accelerated physics
- âœ… Generate photorealistic synthetic training data
- âœ… Implement domain randomization for robust models
- âœ… Export labeled datasets for AI training
- âœ… Integrate Isaac Sim with ROS 2 Humble
- âœ… Optimize simulation performance with GPU acceleration

---

## ğŸ“š Introduction: Why Isaac Sim?

**NVIDIA Isaac Sim** is a physics-simulation engine built on NVIDIA Omniverse. It provides:

- **Photorealism**: Physically-based rendering for realistic training data
- **GPU Acceleration**: 10-100x faster than CPU-based physics
- **Synthetic Data**: Unlimited labeled training data (no human annotation)
- **Domain Randomization**: Automatic texture/lighting variation
- **ROS 2 Integration**: Native Bridge to ROS 2 applications

### Isaac Sim vs. Gazebo

| Feature | Gazebo | Isaac Sim |
|---------|--------|-----------|
| **Physics Engine** | CPU (ODE, Bullet) | GPU (PhysX, Flex) |
| **Rendering** | Basic OpenGL | Photorealistic Ray-Tracing |
| **Synthetic Data** | Basic textures | Photorealistic + randomization |
| **Speed** | Real-time (1x) | 10-100x faster |
| **GPU Support** | Limited | Native CUDA |
| **Cost** | Free | Free (cloud) / paid (desktop) |
| **Learning Curve** | Moderate | Moderate-High |

---

## ğŸš€ Isaac Sim Architecture

### Three-Layer Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Application Layer (Python, C++)       â”‚
â”‚   â”œâ”€ Simulation logic                   â”‚
â”‚   â”œâ”€ ROS 2 Bridge                       â”‚
â”‚   â””â”€ Data export                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Omniverse Core (USD, Fabric Engine)   â”‚
â”‚   â”œâ”€ Asset management                   â”‚
â”‚   â”œâ”€ Physics (PhysX GPU)                â”‚
â”‚   â””â”€ Rendering (RTX ray-tracing)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   GPU Hardware (NVIDIA A100/H100)       â”‚
â”‚   â”œâ”€ CUDA cores for physics             â”‚
â”‚   â””â”€ Tensor cores for ML inference      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¬ Creating a Simulation Scene

### Python Script to Build Isaac Sim Scene

```python
from omni.isaac.kit import SimulationApp
import omni.isaac.core.utils.stage as stage_utils
from omni.isaac.core import World
from omni.isaac.manipulators import SingleArm
import numpy as np

# Initialize simulation
simulation_app = SimulationApp({"headless": False})
world = World(stage_units_in_meters=1.0)

# Add robot (Panda arm by default)
panda = SingleArm(
    prim_path="/panda",
    name="panda",
    usd_path="omniverse://localhost/NVIDIA/Assets/Isaac/2023.2.1/Isaac/Robots/Franka/franka.usd"
)
world.scene.add(panda)

# Add objects
world.scene.add_default_ground_plane()

# Simulation loop
for i in range(1000):
    if simulation_app.is_running():
        world.step(render=True)  # GPU-accelerated step
        positions, orientations = panda.forward_kinematics(panda.get_joint_positions())
        print(f'Step {i}: End-effector at {positions}')
    else:
        break

simulation_app.close()
```

---

## ğŸ¨ Domain Randomization

### Automatic Texture and Lighting Variation

```python
import random

class DomainRandomizer:
    def __init__(self):
        self.textures = [
            "Metal001_1K", "Wood001_1K", "Fabric001_1K",
            "Concrete001_1K", "Leather001_1K"
        ]
        self.colors = [
            (1, 0, 0), (0, 1, 0), (0, 0, 1),
            (1, 1, 0), (1, 0, 1), (0, 1, 1)
        ]

    def randomize_material(self, prim):
        """Apply random texture and color to object."""
        texture = random.choice(self.textures)
        color = random.choice(self.colors)
        roughness = random.uniform(0.2, 0.8)
        metallic = random.choice([0.0, 1.0])

        # Apply to material
        prim.get_material().set_color_attribute(color)
        prim.get_material().set_roughness(roughness)
        prim.get_material().set_metallic(metallic)

    def randomize_lighting(self):
        """Vary lighting intensity and direction."""
        intensity = random.uniform(500, 2000)  # lux
        x_angle = random.uniform(-45, 45)  # degrees
        y_angle = random.uniform(0, 360)

        # Update light in scene
        light = get_omni_light()
        light.set_intensity(intensity)
        light.set_rotation((x_angle, y_angle, 0))

    def randomize_scene(self):
        """Apply all randomizations."""
        for obj in get_scene_objects():
            self.randomize_material(obj)
        self.randomize_lighting()
```

---

## ğŸ“Š Synthetic Data Generation

### Generating Labeled Datasets

```python
from PIL import Image
import json
import os

class SyntheticDataGenerator:
    def __init__(self, output_dir="datasets/"):
        self.output_dir = output_dir
        self.frame_count = 0
        self.annotations = []

    def capture_frame(self, rgb_image, depth_image, bounding_boxes, pose):
        """Save frame with annotations."""
        # Save RGB image
        rgb_path = f'{self.output_dir}/rgb_{self.frame_count:06d}.png'
        Image.fromarray(rgb_image).save(rgb_path)

        # Save depth map
        depth_path = f'{self.output_dir}/depth_{self.frame_count:06d}.npy'
        np.save(depth_path, depth_image)

        # Create annotation
        annotation = {
            'frame_id': self.frame_count,
            'rgb_path': rgb_path,
            'depth_path': depth_path,
            'bounding_boxes': bounding_boxes,  # Object detections
            'camera_pose': pose,  # Camera extrinsics
            'timestamp': time.time()
        }
        self.annotations.append(annotation)

        # Save metadata every 100 frames
        if self.frame_count % 100 == 0:
            with open(f'{self.output_dir}/annotations.json', 'w') as f:
                json.dump(self.annotations, f, indent=2)

        self.frame_count += 1

    def generate_dataset(self, num_frames=10000):
        """Generate complete training dataset."""
        randomizer = DomainRandomizer()

        for i in range(num_frames):
            randomizer.randomize_scene()  # New random environment

            # Capture data
            rgb = world.render_rgb()
            depth = world.render_depth()
            bboxes = world.get_bounding_boxes()
            pose = world.get_camera_pose()

            self.capture_frame(rgb, depth, bboxes, pose)

            if (i + 1) % 100 == 0:
                print(f'Generated {i + 1}/{num_frames} frames')
```

---

## ğŸŒ‰ ROS 2 Integration

### Publishing Sensor Data

```python
import rclpy
from sensor_msgs.msg import Image, PointCloud2
from geometry_msgs.msg import TransformStamped

class IsaacROS2Bridge(rclpy.node.Node):
    def __init__(self):
        super().__init__('isaac_ros_bridge')

        # Publishers
        self.rgb_pub = self.create_publisher(Image, '/camera/rgb', 10)
        self.depth_pub = self.create_publisher(PointCloud2, '/camera/depth', 10)

        # ROS 2 â†” Isaac Sim synchronization
        self.timer = self.create_timer(0.033, self.publish_data)  # 30 Hz

    def publish_data(self):
        """Publish Isaac Sim data to ROS 2 topics."""
        # Render from Isaac Sim
        rgb = world.render_rgb()
        depth = world.render_depth()

        # Convert to ROS 2 messages
        rgb_msg = self.numpy_to_image_msg(rgb)
        depth_msg = self.depth_to_pointcloud_msg(depth)

        # Publish
        self.rgb_pub.publish(rgb_msg)
        self.depth_pub.publish(depth_msg)

    def numpy_to_image_msg(self, cv_image):
        from cv_bridge import CvBridge
        bridge = CvBridge()
        return bridge.cv2_to_imgmsg(cv_image, encoding='bgr8')
```

---

## âš™ï¸ GPU Acceleration Tips

### Performance Optimization

```python
# Use GPU physics (default in Isaac Sim)
physics_context = get_omniverse_physics()
physics_context.set_gpu_enabled(True)

# Batch rendering for multiple cameras
world.set_multi_gpu_enabled(True)

# Async physics updates (non-blocking)
world.step_async()

# Performance monitoring
metrics = world.get_performance_metrics()
print(f'Physics FPS: {metrics["physics_fps"]}')
print(f'Render FPS: {metrics["render_fps"]}')
```

---

## âœ¨ Best Practices

1. **Use Streaming ROS2**: Don't render UI - use headless mode for speed
2. **Batch Randomization**: Apply domain randomization every N frames
3. **Multi-Camera Setup**: Capture different viewpoints simultaneously
4. **Data Validation**: Verify synthetic data matches real data distribution
5. **Version Control**: Track Isaac Sim version and asset versions
6. **Cloud Deployment**: Use NVIDIA Omniverse Cloud for scaling

---

**Next**: [Lesson 3.2 - SLAM and Autonomous Navigation](lesson-3-2-slam.mdx)
